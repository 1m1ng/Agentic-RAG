"""
Agentic RAG ç³»ç»Ÿæ ¸å¿ƒ
å®ç°å®Œæ•´çš„æŸ¥è¯¢é‡å†™ -> æ··åˆæ£€ç´¢ -> é‡æ’åº -> ç”Ÿæˆç­”æ¡ˆ -> è¯„æµ‹æµæ°´çº¿
"""

import asyncio
import os
import json
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from agno.models.openai import OpenAIChat
from agno.agent import Agent
from retriever_hybrid import HybridRetriever
from document_loader import DocxChunk


def get_kimi_client() -> OpenAIChat:
    """
    è·å–é…ç½®å¥½çš„ Kimi å®¢æˆ·ç«¯
    
    Returns:
        é…ç½®å¥½çš„ OpenAIChat å®¢æˆ·ç«¯
    """
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è·å– API å¯†é’¥
    api_key = os.getenv("KIMI_API_KEY")
    
    if not api_key:
        raise ValueError("é”™è¯¯ï¼šæœªæ‰¾åˆ° KIMI_API_KEY ç¯å¢ƒå˜é‡")
    
    # è¿”å›é…ç½®å¥½çš„ Kimi å®¢æˆ·ç«¯
    return OpenAIChat(
        id="kimi-k2-0905-preview",
        api_key=api_key,
        base_url="https://api.moonshot.cn/v1"
    )


async def rewrite_query(client: OpenAIChat, user_query: str) -> list[str]:
    """
    ä½¿ç”¨ Kimi LLM é‡å†™æŸ¥è¯¢ä¸ºå¤šä¸ªå˜ä½“
    
    Args:
        client: Kimi OpenAIChat å®¢æˆ·ç«¯
        user_query: ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢
        
    Returns:
        åŒ…å«åŸå§‹æŸ¥è¯¢å’Œé‡å†™å˜ä½“çš„æŸ¥è¯¢åˆ—è¡¨
    """
    # ç³»ç»Ÿæç¤ºè¯ - æŒ‡ç¤º Kimi å……å½“æŸ¥è¯¢é‡å†™ä¸“å®¶
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡æœç´¢æŸ¥è¯¢é‡å†™ä¸“å®¶ã€‚

ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„å•ä¸ªæŸ¥è¯¢é‡å†™ä¸º 3 ä¸ªä¸åŒçš„å˜ä½“ï¼Œä»¥æé«˜åœ¨æ—…æ¸¸æ‰‹å†Œä¸­çš„å¬å›ç‡ã€‚

é‡å†™æ—¶è¯·è€ƒè™‘ï¼š
1. åŒä¹‰è¯å’Œè¿‘ä¹‰è¯
2. ç¼©å†™å’Œå…¨ç§°
3. ä¸åŒçš„æœç´¢æ„å›¾å’Œè¡¨è¾¾æ–¹å¼
4. ä¿æŒæŸ¥è¯¢çš„è¯­ä¹‰ç›¸å…³æ€§

**ä¸¥æ ¼è¦æ±‚**ï¼šä½ å¿…é¡»åªè¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼Œæ ¼å¼ä¸ºï¼š
{
  "queries": ["query1", "query2", "query3"]
}

ä¸è¦è¿”å›ä»»ä½•å…¶ä»–å†…å®¹ï¼Œåªè¿”å› JSON å¯¹è±¡ã€‚"""

    # ç”¨æˆ·æç¤ºè¯
    user_prompt = f"è¯·é‡å†™è¿™ä¸ªæŸ¥è¯¢ï¼š'{user_query}'"
    
    # åˆ›å»º Agent å®ä¾‹
    agent = Agent(
        model=client,
        markdown=False
    )
    
    # åˆå¹¶æç¤ºè¯
    combined_prompt = f"""{system_prompt}

---

{user_prompt}"""
    
    # è°ƒç”¨ Kimi è¿›è¡ŒæŸ¥è¯¢é‡å†™
    response = agent.run(combined_prompt, stream=False)
    
    # è§£æå“åº”
    try:
        # è·å–å“åº”å†…å®¹
        if hasattr(response, 'content'):
            content = response.content
        else:
            content = str(response)
        
        # ç¡®ä¿ content ä¸ä¸º None
        if content is None:
            content = "{}"
        
        # è§£æ JSON
        result = json.loads(content)
        queries = result.get('queries', [])
        
        # è¿½åŠ åŸå§‹æŸ¥è¯¢åˆ°åˆ—è¡¨ä¸­
        queries.append(user_query)
        
        return queries
        
    except json.JSONDecodeError as e:
        print(f"JSON è§£æé”™è¯¯: {e}")
        print(f"åŸå§‹å“åº”: {content}")
        # å¦‚æœè§£æå¤±è´¥ï¼Œè‡³å°‘è¿”å›åŸå§‹æŸ¥è¯¢
        return [user_query]


async def evaluate_answer(client: OpenAIChat, query: str, generated_answer: str, context: str) -> dict:
    """
    ä½¿ç”¨ Kimi AI è¯„æµ‹ç”Ÿæˆç­”æ¡ˆçš„è´¨é‡
    
    Args:
        client: Kimi OpenAIChat å®¢æˆ·ç«¯
        query: ç”¨æˆ·é—®é¢˜
        generated_answer: ç”Ÿæˆçš„ç­”æ¡ˆ
        context: æä¾›çš„ä¸Šä¸‹æ–‡
        
    Returns:
        åŒ…å« score å’Œ reasoning çš„è¯„æµ‹ç»“æœå­—å…¸
    """
    # ç³»ç»Ÿæç¤ºè¯ - æŒ‡ç¤º Kimi å……å½“è¯„æµ‹å‘˜
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ RAG ç³»ç»Ÿè¯„æµ‹å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯è¯„ä¼° [ç”Ÿæˆç­”æ¡ˆ] çš„è´¨é‡ã€‚

è¯„æµ‹ç»´åº¦ï¼š
1. **å‡†ç¡®æ€§ (Accuracy)**: åˆ¤æ–­ [ç”Ÿæˆç­”æ¡ˆ] æ˜¯å¦æ­£ç¡®å›ç­”äº† [ç”¨æˆ·é—®é¢˜]ã€‚
2. **å¿ å®åº¦ (Faithfulness)**: åˆ¤æ–­ [ç”Ÿæˆç­”æ¡ˆ] ä¸­çš„ä¿¡æ¯æ˜¯å¦**å®Œå…¨**åŸºäº [ä¸Šä¸‹æ–‡]ï¼Œæ²¡æœ‰æé€ æˆ–æ·»åŠ ä¸Šä¸‹æ–‡ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚
3. **å®Œæ•´æ€§ (Completeness)**: åˆ¤æ–­ [ç”Ÿæˆç­”æ¡ˆ] æ˜¯å¦å……åˆ†åˆ©ç”¨äº† [ä¸Šä¸‹æ–‡] ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
- 1.0: å®Œå…¨å‡†ç¡®ã€å®Œå…¨å¿ å®ã€ä¿¡æ¯å®Œæ•´
- 0.7-0.9: åŸºæœ¬å‡†ç¡®ä¸”å¿ å®ï¼Œä½†å¯èƒ½æœ‰è½»å¾®ä¸å®Œæ•´
- 0.4-0.6: éƒ¨åˆ†æ­£ç¡®ä½†æœ‰æ˜æ˜¾é—æ¼æˆ–è½»å¾®åå·®
- 0.0-0.3: é”™è¯¯ç­”æ¡ˆã€ä¸¥é‡æé€ æˆ–ä¸¥é‡åç¦»ä¸Šä¸‹æ–‡

**é‡è¦**ï¼šä½ å¿…é¡»è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- "score": æµ®ç‚¹æ•°ï¼ŒèŒƒå›´ 0.0 åˆ° 1.0
- "reasoning": å­—ç¬¦ä¸²ï¼Œè¯¦ç»†è§£é‡Šè¯„åˆ†ç†ç”±

åªè¿”å› JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

    # ç”¨æˆ·æç¤ºè¯
    user_prompt = f"""è¯·è¯„æµ‹ä»¥ä¸‹ç­”æ¡ˆï¼š

[ç”¨æˆ·é—®é¢˜]
{query}

[ä¸Šä¸‹æ–‡]
{context}

[ç”Ÿæˆç­”æ¡ˆ]
{generated_answer}

è¯·æ ¹æ®å‡†ç¡®æ€§ã€å¿ å®åº¦å’Œå®Œæ•´æ€§è¿›è¡Œè¯„åˆ†ï¼Œå¹¶è¿”å› JSON æ ¼å¼çš„è¯„æµ‹ç»“æœã€‚"""

    # åˆ›å»º Agent å®ä¾‹
    agent = Agent(
        model=client,
        markdown=False
    )
    
    # åˆå¹¶æç¤ºè¯
    combined_prompt = f"""{system_prompt}

---

{user_prompt}"""
    
    # è°ƒç”¨ Kimi è¿›è¡Œè¯„æµ‹
    response = agent.run(combined_prompt, stream=False)
    
    # è§£æå“åº”
    try:
        # è·å–å“åº”å†…å®¹
        if hasattr(response, 'content'):
            content = response.content
        else:
            content = str(response)
        
        # ç¡®ä¿ content ä¸ä¸º None
        if content is None:
            content = "{}"
        
        # è§£æ JSON
        result = json.loads(content)
        return result
        
    except json.JSONDecodeError as e:
        print(f"JSON è§£æé”™è¯¯: {e}")
        print(f"åŸå§‹å“åº”: {content}")
        return {"score": 0.0, "reasoning": "è¯„æµ‹å¤±è´¥ï¼šæ— æ³•è§£æ JSON å“åº”"}


class AgenticRAG:
    """Agentic RAG ç³»ç»Ÿæ ¸å¿ƒç±»"""
    
    def __init__(self, max_workers: int = 4):
        """
        åˆå§‹åŒ– RAG Agent
        
        Args:
            max_workers: çº¿ç¨‹æ± æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        print("æ­£åœ¨åˆå§‹åŒ– RAG Agent...")
        
        # åˆå§‹åŒ– Kimi å®¢æˆ·ç«¯
        self.kimi_client = get_kimi_client()
        
        # åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
        print("æ­£åœ¨åˆå§‹åŒ– Hybrid Retriever (è¿™å¯èƒ½éœ€è¦1-2åˆ†é’Ÿ)...")
        self.retriever = HybridRetriever()
        
        # åˆå§‹åŒ–çº¿ç¨‹æ± 
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        self.chunks_lock = Lock()
        
        print("Agent åˆå§‹åŒ–å®Œæˆã€‚")
    
    def _search_single_query(self, args: tuple) -> list:
        """
        å•ä¸ªæŸ¥è¯¢çš„æ£€ç´¢ï¼ˆç”¨äºå¤šçº¿ç¨‹æ‰§è¡Œï¼‰
        
        Args:
            args: (query_index, query) å…ƒç»„
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        query_index, query = args
        print(f"  - æ­£åœ¨æ£€ç´¢å­æŸ¥è¯¢ {query_index}/[æ€»æ•°]: '{query}'")
        return self.retriever.search(query, top_k=10)
    
    def _parallel_search(self, rewritten_queries: list[str]) -> dict:
        """
        å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæŸ¥è¯¢çš„æ£€ç´¢
        
        Args:
            rewritten_queries: é‡å†™åçš„æŸ¥è¯¢åˆ—è¡¨
            
        Returns:
            å»é‡åçš„å€™é€‰å—å­—å…¸
        """
        all_candidate_chunks: dict = {}
        
        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = [(i + 1, query) for i, query in enumerate(rewritten_queries)]
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œæ£€ç´¢
        futures = []
        for task in tasks:
            future = self.thread_pool.submit(self._search_single_query, task)
            futures.append(future)
        
        # æ”¶é›†ç»“æœå¹¶å»é‡
        for future in as_completed(futures):
            try:
                results = future.result()
                for chunk in results:
                    para_id = chunk['metadata']['paragraph_id']
                    with self.chunks_lock:
                        all_candidate_chunks[para_id] = chunk
            except Exception as e:
                print(f"æ£€ç´¢ä»»åŠ¡å‡ºé”™: {e}")
        
        return all_candidate_chunks
    
    async def run(self, user_query: str, enable_evaluation: bool = True):
        """
        æ‰§è¡Œå®Œæ•´çš„ RAG æµæ°´çº¿ï¼ˆä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿï¼‰
        
        Args:
            user_query: ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢
            enable_evaluation: æ˜¯å¦å¯ç”¨ç­”æ¡ˆè¯„æµ‹
        """
        print(f"\n{'=' * 80}")
        print(f"ç”¨æˆ·é—®é¢˜: {user_query}")
        print(f"{'=' * 80}\n")
        
        # ==================== æ­¥éª¤ 1: æŸ¥è¯¢é‡å†™ ====================
        print("--- æ­¥éª¤ 1: Agent æ­£åœ¨é‡å†™æŸ¥è¯¢ ---")
        rewritten_queries = await rewrite_query(self.kimi_client, user_query)
        
        for idx, query in enumerate(rewritten_queries, 1):
            print(f"  {idx}. {query}")
        print()
        
        # ==================== æ­¥éª¤ 2: æ··åˆæ£€ç´¢ï¼ˆå¤šçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œï¼‰ ====================
        print(f"--- æ­¥éª¤ 2: Agent æ­£åœ¨æ‰§è¡Œæ··åˆæ£€ç´¢ï¼ˆä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç† {len(rewritten_queries)} ä¸ªæŸ¥è¯¢ï¼‰---")
        
        # å¹¶è¡Œæ‰§è¡Œæ£€ç´¢
        all_candidate_chunks = self._parallel_search(rewritten_queries)
        
        # è·å–æ‰€æœ‰å”¯ä¸€å€™é€‰å—
        final_candidates = list(all_candidate_chunks.values())
        print(f"\n--- æ­¥éª¤ 3: Agent æ­£åœ¨å¯¹ {len(final_candidates)} ä¸ªå”¯ä¸€å€™é€‰å—è¿›è¡Œæœ€ç»ˆé‡æ’ ---")
        
        # ==================== æ­¥éª¤ 3: æœ€ç»ˆé‡æ’ ====================
        # ä½¿ç”¨åŸå§‹æŸ¥è¯¢å¯¹æ‰€æœ‰å€™é€‰å—è¿›è¡Œæœ€ç»ˆé‡æ’
        pairs = [[user_query, chunk['text']] for chunk in final_candidates]
        scores = self.retriever.reranker.compute_score(pairs, batch_size=4)  # type: ignore
        
        # ç»„åˆåˆ†æ•°å’Œæ–‡æ¡£å—
        scored_chunks = list(zip(scores, final_candidates))  # type: ignore
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        scored_chunks.sort(key=lambda x: x[0], reverse=True)  # type: ignore
        
        # è·å– top 3 ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£å—
        top_k_chunks = [chunk for score, chunk in scored_chunks[:3]]  # type: ignore
        
        # æ‰“å°æœ€ç»ˆæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        print("\n--- æœ€ç»ˆæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ (Context) ---\n")
        context_str = ""
        
        for idx, chunk in enumerate(top_k_chunks, 1):
            print(f"ä¸Šä¸‹æ–‡å— {idx}:")
            print(f"  æ–‡æœ¬: {chunk['text']}")
            print(f"  å…ƒæ•°æ®: {chunk['metadata']}")
            print()
            
            # æ‹¼æ¥ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
            context_str += f"[æ–‡æ¡£ç‰‡æ®µ {idx}]\n{chunk['text']}\n\n"
        
        
        # ==================== æ­¥éª¤ 4: ç”Ÿæˆç­”æ¡ˆ ====================
        print("--- æ­¥éª¤ 4: Agent æ­£åœ¨åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ ---\n")
        
        # ç³»ç»Ÿæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„ [ä¸Šä¸‹æ–‡]ï¼Œç”¨ä¸­æ–‡å›ç­” [ç”¨æˆ·é—®é¢˜]ã€‚

ä½ çš„å›ç­”å¿…é¡»ä¸¥æ ¼åŸºäº [ä¸Šä¸‹æ–‡] åŒ…å«çš„ä¿¡æ¯ï¼Œç¦æ­¢æé€ ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""
        
        # ç”¨æˆ·æç¤ºè¯
        user_prompt = f"""[ä¸Šä¸‹æ–‡]:
{context_str}

[ç”¨æˆ·é—®é¢˜]:
{user_query}"""
        
        # åˆ›å»º Agent å®ä¾‹å¹¶æµå¼ç”Ÿæˆç­”æ¡ˆ
        agent = Agent(
            model=self.kimi_client,
            markdown=False
        )
        
        # åˆå¹¶æç¤ºè¯
        combined_prompt = f"""{system_prompt}

---

{user_prompt}"""
        
        # æ‰“å°æœ€ç»ˆç­”æ¡ˆæ ‡é¢˜
        print("--- æœ€ç»ˆç­”æ¡ˆ ---\n")
        
        # æµå¼æ‰“å°ç­”æ¡ˆå¹¶æ”¶é›†å®Œæ•´ç­”æ¡ˆ
        final_answer = ""
        for chunk in agent.run(combined_prompt, stream=True):
            if hasattr(chunk, 'content'):
                content = chunk.content
            else:
                content = str(chunk)
            
            if content:
                print(content, end="", flush=True)
                final_answer += content
        
        print(f"\n\n{'=' * 80}\n")
        
        # ==================== æ­¥éª¤ 5: è¯„æµ‹ç­”æ¡ˆ ====================
        if enable_evaluation:
            print("--- æ­¥éª¤ 5: Agent æ­£åœ¨è¯„æµ‹ç­”æ¡ˆè´¨é‡ ---\n")
            
            evaluation = await evaluate_answer(
                self.kimi_client,
                user_query,
                final_answer,
                context_str
            )
            
            print(f"ğŸ“Š è¯„æµ‹ç»“æœ:")
            print(f"   å¾—åˆ†: {evaluation.get('score', 0.0):.2f}")
            print(f"   ç†ç”±: {evaluation.get('reasoning', 'æ— ')}")
            print(f"\n{'=' * 80}\n")
        
        return final_answer
    
    def close(self):
        """å…³é—­çº¿ç¨‹æ± èµ„æº"""
        if hasattr(self, 'thread_pool'):
            self.thread_pool.shutdown(wait=True)
            print("\nçº¿ç¨‹æ± å·²å…³é—­")


async def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œ Agentic RAG ç³»ç»Ÿ"""
    # åˆå§‹åŒ– RAG Agentï¼ˆè®¾ç½® max_workers=4 è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼‰
    rag = AgenticRAG(max_workers=4)
    
    try:
        # å®šä¹‰æµ‹è¯•æŸ¥è¯¢
        test_query = "å…‰æ˜åŒºæœ‰ä»€ä¹ˆæ–‡åŒ–å»ºç­‘ï¼Ÿå› ä»€ä¹ˆè€Œé—»åï¼Ÿ"
        
        # è¿è¡Œå®Œæ•´çš„ RAG æµæ°´çº¿
        await rag.run(test_query)
    finally:
        # ç¡®ä¿å…³é—­çº¿ç¨‹æ± èµ„æº
        rag.close()


if __name__ == "__main__":
    asyncio.run(main())
