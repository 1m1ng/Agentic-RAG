"""
æ‰¹é‡è¯„æµ‹è„šæœ¬
è¯»å–è¯„æµ‹é›† Excel æ–‡ä»¶ï¼Œè¿è¡Œ RAG Agentï¼Œå¹¶å°†ç»“æœä¸æ ‡å‡†ç­”æ¡ˆå¯¹æ¯”è¯„åˆ†
"""

import asyncio
import json
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
from agno.models.openai import OpenAIChat
from agno.agent import Agent
from rag_agent import AgenticRAG, get_kimi_client


async def evaluate_with_standard(
    client: OpenAIChat, 
    query: str, 
    generated_answer: str, 
    standard_answer: str,
    context: str
) -> dict:
    """
    ä½¿ç”¨ Kimi AI è¯„æµ‹ç”Ÿæˆç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆçš„åŒ¹é…åº¦
    
    Args:
        client: Kimi OpenAIChat å®¢æˆ·ç«¯
        query: ç”¨æˆ·é—®é¢˜
        generated_answer: RAG ç”Ÿæˆçš„ç­”æ¡ˆ
        standard_answer: æ ‡å‡†ç­”æ¡ˆ
        context: RAG ä½¿ç”¨çš„ä¸Šä¸‹æ–‡
        
    Returns:
        åŒ…å« score å’Œ reasoning çš„è¯„æµ‹ç»“æœå­—å…¸
    """
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ RAG ç³»ç»Ÿè¯„æµ‹å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯è¯„ä¼° [RAGç”Ÿæˆç­”æ¡ˆ] ç›¸å¯¹äº [æ ‡å‡†ç­”æ¡ˆ] çš„è´¨é‡ã€‚

è¯„æµ‹ç»´åº¦ï¼š
1. **å‡†ç¡®æ€§ (Accuracy)**: [RAGç”Ÿæˆç­”æ¡ˆ] æ˜¯å¦æ­£ç¡®å›ç­”äº† [ç”¨æˆ·é—®é¢˜]ï¼Œä¸ [æ ‡å‡†ç­”æ¡ˆ] çš„ä¸€è‡´æ€§å¦‚ä½•ã€‚
2. **å¿ å®åº¦ (Faithfulness)**: [RAGç”Ÿæˆç­”æ¡ˆ] æ˜¯å¦åŸºäº [ä¸Šä¸‹æ–‡]ï¼Œæ²¡æœ‰æé€ ä¿¡æ¯ã€‚
3. **å®Œæ•´æ€§ (Completeness)**: [RAGç”Ÿæˆç­”æ¡ˆ] æ˜¯å¦åŒ…å«äº† [æ ‡å‡†ç­”æ¡ˆ] ä¸­çš„å…³é”®ä¿¡æ¯ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
- 1.0: ä¸æ ‡å‡†ç­”æ¡ˆå®Œå…¨ä¸€è‡´æˆ–æ›´å¥½ï¼Œå‡†ç¡®ä¸”å®Œæ•´
- 0.8-0.9: ä¸æ ‡å‡†ç­”æ¡ˆåŸºæœ¬ä¸€è‡´ï¼Œæœ‰è½»å¾®å·®å¼‚ä½†ä¸å½±å“å‡†ç¡®æ€§
- 0.6-0.7: åŒ…å«æ ‡å‡†ç­”æ¡ˆçš„ä¸»è¦å†…å®¹ï¼Œä½†æœ‰æ˜æ˜¾é—æ¼æˆ–ä¸å¤Ÿå‡†ç¡®
- 0.4-0.5: éƒ¨åˆ†æ­£ç¡®ï¼Œä½†æœ‰é‡è¦ä¿¡æ¯ç¼ºå¤±æˆ–åå·®
- 0.0-0.3: ä¸æ ‡å‡†ç­”æ¡ˆä¸¥é‡ä¸ç¬¦ï¼Œé”™è¯¯æˆ–ä¸¥é‡ä¸å®Œæ•´

**é‡è¦**ï¼šä½ å¿…é¡»è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- "score": æµ®ç‚¹æ•°ï¼ŒèŒƒå›´ 0.0 åˆ° 1.0
- "reasoning": å­—ç¬¦ä¸²ï¼Œè¯¦ç»†è§£é‡Šè¯„åˆ†ç†ç”±

åªè¿”å› JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

    user_prompt = f"""è¯·è¯„æµ‹ä»¥ä¸‹ç­”æ¡ˆï¼š

[ç”¨æˆ·é—®é¢˜]
{query}

[æ ‡å‡†ç­”æ¡ˆ]
{standard_answer}

[RAGç”Ÿæˆç­”æ¡ˆ]
{generated_answer}

[ä¸Šä¸‹æ–‡]
{context}

è¯·æ ¹æ®å‡†ç¡®æ€§ã€å¿ å®åº¦å’Œå®Œæ•´æ€§è¿›è¡Œè¯„åˆ†ï¼Œå¹¶è¿”å› JSON æ ¼å¼çš„è¯„æµ‹ç»“æœã€‚"""

    # åˆ›å»º Agent å®ä¾‹
    agent = Agent(
        model=client,
        markdown=False
    )
    
    # åˆå¹¶æç¤ºè¯
    combined_prompt = f"""{system_prompt}

---

{user_prompt}"""
    
    # è°ƒç”¨ Kimi è¿›è¡Œè¯„æµ‹
    response = agent.run(combined_prompt, stream=False)
    
    # è§£æå“åº”
    try:
        if hasattr(response, 'content'):
            content = response.content
        else:
            content = str(response)
        
        if content is None:
            content = "{}"
        
        result = json.loads(content)
        return result
        
    except json.JSONDecodeError as e:
        print(f"JSON è§£æé”™è¯¯: {e}")
        print(f"åŸå§‹å“åº”: {content}")
        return {"score": 0.0, "reasoning": "è¯„æµ‹å¤±è´¥ï¼šæ— æ³•è§£æ JSON å“åº”"}


async def run_rag_and_evaluate(
    rag: AgenticRAG,
    kimi_client: OpenAIChat,
    query: str,
    standard_answer: str
) -> dict:
    """
    è¿è¡Œ RAG å¹¶è¯„æµ‹ç»“æœ
    
    Args:
        rag: RAG Agent å®ä¾‹
        kimi_client: Kimi å®¢æˆ·ç«¯
        query: ç”¨æˆ·é—®é¢˜
        standard_answer: æ ‡å‡†ç­”æ¡ˆ
        
    Returns:
        åŒ…å«ç”Ÿæˆç­”æ¡ˆã€è¯„æµ‹ç»“æœç­‰ä¿¡æ¯çš„å­—å…¸
    """
    print(f"\n{'=' * 100}")
    print(f"æ­£åœ¨å¤„ç†æŸ¥è¯¢: {query}")
    print(f"{'=' * 100}\n")
    
    # æ‰§è¡Œ RAGï¼ˆç¦ç”¨è‡ªåŠ¨è¯„æµ‹ï¼‰
    try:
        # ä¿®æ”¹ run æ–¹æ³•ä»¥è¿”å›æ›´å¤šä¿¡æ¯
        from rag_agent import rewrite_query
        
        # 1. æŸ¥è¯¢é‡å†™
        print("--- æ­¥éª¤ 1: æŸ¥è¯¢é‡å†™ ---")
        rewritten_queries = await rewrite_query(kimi_client, query)
        print(f"é‡å†™æŸ¥è¯¢æ•°: {len(rewritten_queries)}")
        
        # 2. æ··åˆæ£€ç´¢
        print("\n--- æ­¥éª¤ 2: æ··åˆæ£€ç´¢ ---")
        from document_loader import DocxChunk
        all_candidate_chunks: dict[int, DocxChunk] = {}
        
        for i, q in enumerate(rewritten_queries, 1):
            results = rag.retriever.search(q, top_k=10)
            for chunk in results:
                para_id = chunk['metadata']['paragraph_id']
                all_candidate_chunks[para_id] = chunk
        
        final_candidates = list(all_candidate_chunks.values())
        print(f"å€™é€‰å—æ•°: {len(final_candidates)}")
        
        # 3. æœ€ç»ˆé‡æ’
        print("\n--- æ­¥éª¤ 3: æœ€ç»ˆé‡æ’ ---")
        pairs = [[query, chunk['text']] for chunk in final_candidates]
        scores = rag.retriever.reranker.compute_score(pairs, batch_size=4)  # type: ignore
        scored_chunks = list(zip(scores, final_candidates))  # type: ignore
        scored_chunks.sort(key=lambda x: x[0], reverse=True)  # type: ignore
        top_k_chunks = [chunk for score, chunk in scored_chunks[:3]]  # type: ignore
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_str = ""
        for idx, chunk in enumerate(top_k_chunks, 1):
            context_str += f"[æ–‡æ¡£ç‰‡æ®µ {idx}]\n{chunk['text']}\n\n"
        
        print(f"æœ€ç»ˆä¸Šä¸‹æ–‡å—æ•°: {len(top_k_chunks)}")
        
        # 4. ç”Ÿæˆç­”æ¡ˆ
        print("\n--- æ­¥éª¤ 4: ç”Ÿæˆç­”æ¡ˆ ---")
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„ [ä¸Šä¸‹æ–‡]ï¼Œç”¨ä¸­æ–‡å›ç­” [ç”¨æˆ·é—®é¢˜]ã€‚

ä½ çš„å›ç­”å¿…é¡»ä¸¥æ ¼åŸºäº [ä¸Šä¸‹æ–‡] åŒ…å«çš„ä¿¡æ¯ï¼Œç¦æ­¢æé€ ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""
        
        user_prompt = f"""[ä¸Šä¸‹æ–‡]:
{context_str}

[ç”¨æˆ·é—®é¢˜]:
{query}"""
        
        agent = Agent(
            model=kimi_client,
            markdown=False
        )
        
        combined_prompt = f"""{system_prompt}

---

{user_prompt}"""
        
        # éæµå¼è·å–å®Œæ•´ç­”æ¡ˆ
        response = agent.run(combined_prompt, stream=False)
        
        if hasattr(response, 'content'):
            generated_answer = response.content or ""
        else:
            generated_answer = str(response)
        
        if not generated_answer:
            generated_answer = ""
        
        print(f"ç”Ÿæˆç­”æ¡ˆé•¿åº¦: {len(generated_answer)} å­—ç¬¦")
        
        # 5. è¯„æµ‹
        print("\n--- æ­¥éª¤ 5: è¯„æµ‹ç­”æ¡ˆ ---")
        evaluation = await evaluate_with_standard(
            kimi_client,
            query,
            generated_answer,
            standard_answer,
            context_str
        )
        
        print(f"è¯„æµ‹å¾—åˆ†: {evaluation.get('score', 0.0):.2f}")
        print(f"è¯„æµ‹ç†ç”±: {evaluation.get('reasoning', 'æ— ')[:100]}...")
        
        return {
            "query": query,
            "standard_answer": standard_answer,
            "generated_answer": generated_answer,
            "context": context_str,
            "rewritten_queries_count": len(rewritten_queries),
            "candidate_chunks_count": len(final_candidates),
            "score": evaluation.get('score', 0.0),
            "reasoning": evaluation.get('reasoning', 'æ— '),
            "status": "æˆåŠŸ"
        }
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
        return {
            "query": query,
            "standard_answer": standard_answer,
            "generated_answer": "",
            "context": "",
            "rewritten_queries_count": 0,
            "candidate_chunks_count": 0,
            "score": 0.0,
            "reasoning": f"å¤„ç†å¤±è´¥: {str(e)}",
            "status": "å¤±è´¥"
        }


async def batch_evaluate(input_file: str = "è¯„æµ‹é›†.xlsx", output_file: str | None = None):
    """
    æ‰¹é‡è¯„æµ‹ä¸»å‡½æ•°
    
    Args:
        input_file: è¾“å…¥çš„è¯„æµ‹é›† Excel æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºçš„ç»“æœ Excel æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼‰
    """
    print("=" * 100)
    print("æ‰¹é‡è¯„æµ‹å¼€å§‹")
    print("=" * 100)
    
    # è¯»å–è¯„æµ‹é›†
    print(f"\næ­£åœ¨è¯»å–è¯„æµ‹é›†: {input_file}")
    try:
        df = pd.read_excel(input_file)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ {input_file}")
        return
    
    # æ£€æŸ¥åˆ—å
    if 'query' not in df.columns or 'standard_answer' not in df.columns:
        print("âŒ é”™è¯¯ï¼šExcel æ–‡ä»¶å¿…é¡»åŒ…å« 'query' å’Œ 'standard_answer' åˆ—")
        print(f"å½“å‰åˆ—å: {df.columns.tolist()}")
        return
    
    print(f"âœ… æˆåŠŸè¯»å– {len(df)} æ¡è¯„æµ‹æ•°æ®")
    
    # åˆå§‹åŒ– RAG Agent
    print("\næ­£åœ¨åˆå§‹åŒ– RAG Agent...")
    rag = AgenticRAG()
    kimi_client = get_kimi_client()
    
    # å­˜å‚¨ç»“æœ
    results = []
    
    # é€æ¡å¤„ç†
    for idx, row in df.iterrows():
        query = str(row['query'])
        standard_answer = str(row['standard_answer'])
        
        print(f"\n{'#' * 100}")
        print(f"è¿›åº¦: {int(idx) + 1}/{len(df)}")  # type: ignore
        print(f"{'#' * 100}")
        
        # è¿è¡Œ RAG å¹¶è¯„æµ‹
        result = await run_rag_and_evaluate(
            rag,
            kimi_client,
            query,
            standard_answer
        )
        
        results.append(result)
    
    # åˆ›å»ºç»“æœ DataFrame
    results_df = pd.DataFrame(results)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if output_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"è¯„æµ‹ç»“æœ_{timestamp}.xlsx"
    
    # ä¿å­˜ç»“æœ
    print(f"\næ­£åœ¨ä¿å­˜ç»“æœåˆ°: {output_file}")
    results_df.to_excel(output_file, index=False)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 100)
    print("æ‰¹é‡è¯„æµ‹å®Œæˆ")
    print("=" * 100)
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»é—®é¢˜æ•°: {len(results)}")
    print(f"   æˆåŠŸå¤„ç†: {sum(1 for r in results if r['status'] == 'æˆåŠŸ')}")
    print(f"   å¤„ç†å¤±è´¥: {sum(1 for r in results if r['status'] == 'å¤±è´¥')}")
    print(f"   å¹³å‡å¾—åˆ†: {results_df['score'].mean():.2f}")
    print(f"   æœ€é«˜å¾—åˆ†: {results_df['score'].max():.2f}")
    print(f"   æœ€ä½å¾—åˆ†: {results_df['score'].min():.2f}")
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("=" * 100)


async def main():
    """ä¸»å‡½æ•°"""
    # æ‰¹é‡è¯„æµ‹
    await batch_evaluate(
        input_file="è¯„æµ‹é›†.xlsx",
        output_file=None  # è‡ªåŠ¨ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
    )


if __name__ == "__main__":
    asyncio.run(main())
