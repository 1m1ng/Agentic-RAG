"""
RAG ç³»ç»Ÿè¯„æµ‹è„šæœ¬
ä½¿ç”¨ Kimi AI ä½œä¸ºè¯„æµ‹å‘˜ï¼Œè¯„ä¼°ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œå¿ å®åº¦
"""

import asyncio
import json
import os
from dotenv import load_dotenv
from agno.models.openai import OpenAIChat
from eval_questions import EVAL_QUESTIONS


async def evaluate_answer(query: str, ground_truth: str, actual_answer: str) -> dict:
    """
    ä½¿ç”¨ Kimi AI è¯„æµ‹ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œå¿ å®åº¦
    
    Args:
        query: ç”¨æˆ·é—®é¢˜
        ground_truth: æ ‡å‡†ç­”æ¡ˆ
        actual_answer: å®é™…ç”Ÿæˆçš„ç­”æ¡ˆ
        
    Returns:
        åŒ…å« score å’Œ reasoning çš„è¯„æµ‹ç»“æœå­—å…¸
    """
    # ç³»ç»Ÿæç¤ºè¯ - æŒ‡ç¤º Kimi å……å½“è¯„æµ‹å‘˜
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ RAG ç³»ç»Ÿè¯„æµ‹å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ® [æ ‡å‡†ç­”æ¡ˆ] è¯„ä¼° [ç”Ÿæˆç­”æ¡ˆ] çš„è´¨é‡ã€‚

è¯„æµ‹ç»´åº¦ï¼š
1. **å‡†ç¡®æ€§ (Accuracy)**: åˆ¤æ–­ [ç”Ÿæˆç­”æ¡ˆ] æ˜¯å¦æ­£ç¡®å›ç­”äº† [ç”¨æˆ·é—®é¢˜]ã€‚
2. **å¿ å®åº¦ (Faithfulness)**: åˆ¤æ–­ [ç”Ÿæˆç­”æ¡ˆ] ä¸­çš„ä¿¡æ¯æ˜¯å¦**å®Œå…¨**åŒ…å«åœ¨ [æ ‡å‡†ç­”æ¡ˆ] ä¸­ï¼Œæ²¡æœ‰æé€ æˆ–æ·»åŠ é¢å¤–ä¿¡æ¯ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
- 1.0: å®Œå…¨å‡†ç¡®ä¸”å®Œå…¨å¿ å®
- 0.7-0.9: åŸºæœ¬å‡†ç¡®ä½†å¯èƒ½ä¸å®Œæ•´ï¼Œæˆ–æœ‰è½»å¾®åå·®
- 0.4-0.6: éƒ¨åˆ†æ­£ç¡®ä½†æœ‰æ˜æ˜¾é—æ¼æˆ–åå·®
- 0.0-0.3: é”™è¯¯ç­”æ¡ˆæˆ–ä¸¥é‡åç¦»æ ‡å‡†ç­”æ¡ˆ

**é‡è¦**ï¼šä½ å¿…é¡»è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- "score": æµ®ç‚¹æ•°ï¼ŒèŒƒå›´ 0.0 åˆ° 1.0
- "reasoning": å­—ç¬¦ä¸²ï¼Œè¯¦ç»†è§£é‡Šè¯„åˆ†ç†ç”±

åªè¿”å› JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

    # ç”¨æˆ·æç¤ºè¯ - åŒ…å«å…·ä½“çš„é—®é¢˜å’Œç­”æ¡ˆ
    user_prompt = f"""è¯·è¯„æµ‹ä»¥ä¸‹ç­”æ¡ˆï¼š

[ç”¨æˆ·é—®é¢˜]
{query}

[æ ‡å‡†ç­”æ¡ˆ]
{ground_truth}

[ç”Ÿæˆç­”æ¡ˆ]
{actual_answer}

è¯·æ ¹æ®å‡†ç¡®æ€§å’Œå¿ å®åº¦è¿›è¡Œè¯„åˆ†ï¼Œå¹¶è¿”å› JSON æ ¼å¼çš„è¯„æµ‹ç»“æœã€‚"""

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # å®ä¾‹åŒ– OpenAIChat æ¨¡å‹ï¼Œè¿æ¥åˆ° Kimi API
    kimi_model = OpenAIChat(
        id="kimi-k2-0905-preview",
        api_key=os.getenv("KIMI_API_KEY"),
        base_url="https://api.moonshot.cn/v1"
    )
    
    # åˆ›å»º Agent å®ä¾‹
    from agno.agent import Agent
    
    agent = Agent(
        model=kimi_model,
        markdown=False
    )
    
    # åˆå¹¶ system prompt å’Œ user prompt
    combined_prompt = f"""{system_prompt}

---

{user_prompt}"""
    
    # è°ƒç”¨ Kimi API è¿›è¡Œè¯„æµ‹
    response = agent.run(combined_prompt, stream=False)
    
    # è§£æå“åº”å†…å®¹
    try:
        # è·å–å“åº”æ–‡æœ¬å†…å®¹
        if hasattr(response, 'content'):
            content = response.content
        else:
            content = str(response)
        
        # ç¡®ä¿ content ä¸ä¸º None
        if content is None:
            content = ""
        
        result = json.loads(content)
        return result
    except json.JSONDecodeError as e:
        print(f"JSON è§£æé”™è¯¯: {e}")
        print(f"åŸå§‹å“åº”: {content}")
        return {"score": 0.0, "reasoning": "è¯„æµ‹å¤±è´¥ï¼šæ— æ³•è§£æ JSON å“åº”"}


async def main():
    """ä¸»å‡½æ•°ï¼šéå†è¯„ä¼°é—®é¢˜é›†å¹¶è¿›è¡Œè¯„æµ‹"""
    print("=" * 100)
    print("å¼€å§‹è¯„æµ‹ RAG ç³»ç»Ÿ")
    print("=" * 100)
    
    total_score = 0.0
    
    for idx, item in enumerate(EVAL_QUESTIONS, 1):
        print(f"\n{'=' * 100}")
        print(f"è¯„æµ‹é—®é¢˜ {idx}/{len(EVAL_QUESTIONS)}")
        print(f"{'=' * 100}")
        
        query = item["query"]
        ground_truth = item["ground_truth"]
        simulated_answer = item["simulated_answer"]
        
        print(f"\nğŸ“ ç”¨æˆ·é—®é¢˜:")
        print(f"   {query}")
        
        print(f"\nâœ… æ ‡å‡†ç­”æ¡ˆ:")
        print(f"   {ground_truth}")
        
        print(f"\nğŸ¤– æ¨¡æ‹Ÿç­”æ¡ˆ:")
        print(f"   {simulated_answer}")
        
        print(f"\nâ³ æ­£åœ¨è°ƒç”¨ Kimi AI è¯„æµ‹å‘˜...")
        
        # è°ƒç”¨è¯„æµ‹å‡½æ•°
        evaluation = await evaluate_answer(query, ground_truth, simulated_answer)
        
        print(f"\nğŸ“Š è¯„æµ‹ç»“æœ:")
        print(f"   å¾—åˆ†: {evaluation.get('score', 0.0):.2f}")
        print(f"   ç†ç”±: {evaluation.get('reasoning', 'æ— ')}")
        
        total_score += evaluation.get('score', 0.0)
    
    # è®¡ç®—å¹³å‡åˆ†
    avg_score = total_score / len(EVAL_QUESTIONS) if EVAL_QUESTIONS else 0.0
    
    print(f"\n{'=' * 100}")
    print(f"è¯„æµ‹å®Œæˆ")
    print(f"{'=' * 100}")
    print(f"æ€»é—®é¢˜æ•°: {len(EVAL_QUESTIONS)}")
    print(f"å¹³å‡å¾—åˆ†: {avg_score:.2f}")
    print(f"{'=' * 100}\n")


if __name__ == "__main__":
    asyncio.run(main())
